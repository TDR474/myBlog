---
title: 'Naive Bayes and Logistic Regression: A Deep Dive (WIP)'
subtitle:
date: '2021-04-15T08:00:00.000Z'
updated: '2024-06-04T08:00:00.000Z'
categories: []
slug: naive-bayes-logistic
type: 'blogPost'
featured: false
---

<Callout label="Before you start" variant="info">
  If you have trouble viewing the math equations, install 
  [TEX All Things](https://chromewebstore.google.com/detail/tex-all-the-things/cbimabofgmfdkicghcadidpemeenbffn?hl=en).
</Callout>

For Naive Bayes, we are assuming each feature is independent of each other in a common cause Bayes Net configuration, i.e., given a sample of $m$ words and $n$ features:

<div className="math-display">
  {`\\begin{align}
  P(Y, F) = \\underbrace{P(Y)}_{\\text{prior}} \\prod_{i=1}^n \\underbrace{P(f_i|Y)}_{\\text{likelihood}}; \\quad \\forall f_i \\in F, i \\neq j, f_i \\perp f_j | Y;
  \\end{align}`}
</div>

where 
<div className="math-display">
  {`\\begin{align}
  F=\\{f_{1}, f_{2}, f_{3}\\}, Y \\in \\{1, 0\\}
  \\end{align}`}
</div>

Note our **parameter (the prior and likelihoods)** are calculated from the training data using maximum likelihood/relative frequencies:

<div className="math-display">
  {`\\begin{align}
  \\theta_{NB} = \\{\\{P(Y_j)\\}, \\{P(f_i|Y_j)\\}_{i=1}^m\\}^{|Y|}_{j=0}
  \\end{align}`}
</div>

<div className="math-display">
  {`\\begin{align}
  P(Y_j) = \\frac{\\text{count}(Y_j)}{\\text{count}(Y)} \\
  P(f_i|Y_j) = \\frac{\\text{count}(f_i, Y_j)}{\\Sigma_i \\text{count}(f_i, Y)}
  \\end{align}`}
</div>

Inference for our Naive Bayes is done by first computing the posterior from the calculated joint:

<div className="math-display">
  {`\\begin{align}
  \\underbrace{P(Y|F)}_{\\text{posterior}} = \\frac{P(Y, F)}{P(F)}, \\quad P(F) = \\sum_{i=1}^{|Y|} P(Y_i, F)
  \\end{align}`}
</div>

Then our Naive Bayes prediction $Y$ for our words is found by maximizing the posterior (note $P(F)$ is a normalizing constant), or maximizing the joint distribution:

<div className="math-display">
  {`\\begin{align} 
  Y_{NB} &= \\operatorname*{arg\\,max}_Y P(Y | F) \\\\ 
  &= \\operatorname*{arg\\,max}_Y P(Y, F)\\\\ 
  &= \\operatorname*{arg\\,max}_Y P(Y) \\prod_{i=1}^n P(f_i | Y) 
  \\end{align}`}
</div>

Now for logistic regressions, we are stipulating that the following parametric representation; let $Z = F\theta$:

<div className="math-display">
  {`\\begin{align}
  P(Y=1|F) = \\sigma(Z)=\\frac{e^Z}{1+e^Z}
  \\end{align}`}
</div>

Where $\sigma(*)$ is the sigmoidal function that models the conditional class probability. $\theta$ is a $n \times 1$ vector of parameters.

Instead of using relative frequencies from training data, Logistic regression's maximum likelihood finds the parameters $\theta$ that maximizes the likelihood function $L(\theta)$:

<div className="math-display">
  {`\\begin{align}
  L(\\theta) = \\prod_{i=1}^n P(f_i|Y=1) \\prod_{i=1}^n P(f_i|Y=0)\\
  \\theta_{LR} = \\operatorname*{arg\\,max}_\\theta L(\\theta)
  \\end{align}`}
</div>

Inference is done by directly calculating the $Y = \sigma (F \theta)$, if $Y$> $0.5$, we set $Y=1$.

Now it's clear that Naive Bayes is a **simpler model** than Logistic regression. The conditional independence assumption stipulates that each feature is conditionally independent of each other given the class, which may not be true. In contrast, logistic regression does not make such an assumption and instead stipulates a sigmoidal relation between class labels. The parameters of logistic regression are also more flexible, allowing parameters that are not directly calculated from the training dataset. From the design of both algorithms, we can see that Naive Bayes may have higher recall but can suffer from a higher false positive rate, affecting precision. Logistic Regression allows for more control over the trade-off between precision and recall through its objective function.

Logistic regression has its own host of issues as well, though. If features are highly correlated, the variance inflation factor from $F$ will be high. The issue of **multicollinearity** is more pronounced if our library uses Newton-Ralphson instead of vanilla gradient descent, which requires us to calculate the Hessian. If $F$ is collinear, it's non-singular and thus non-invertible, leading to unstable coefficient estimates.

The issue of multicollinearity is more pronounced when we take into account statistical significance. I don't think scikit learn filters based on significance (which itself could be an issue), but when we do, the standard errors we get from the regression won't be correct. A high VIF leads to decreased statistical power, and thus incorrect predictions.
